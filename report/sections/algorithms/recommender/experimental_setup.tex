\subsubsection{Experimental Setup}

\paragraph{Synthetic Social Network Generation.}
All experiments were conducted on synthetic social networks generated using a combination of graph generation and attribute assignment:

\begin{enumerate}
    \item \textbf{Base graph:} Erd\H{o}s--R\'enyi random graph \(G(n, p)\) where \(n\) is the number of users and \(p\) is the friendship probability.
    
    \item \textbf{Personality tags:} Each user is assigned a random subset of tags from a predefined set \(\Sigma\).
    In our experiments, \(\Sigma = \{\)``sports'', ``music'', ``tech'', ``travel'', ``food'', ``art'', ``gaming''\(\}\).
    Each user receives 1--3 randomly selected tags to simulate diverse interests.
\end{enumerate}

This synthetic approach allows controlled experimentation with:
\begin{itemize}
    \item Graph size (\(n\))
    \item Density (\(p\))
    \item Tag distribution
    \item Train/test splits
    \item Noise injection
\end{itemize}

Random generation uses fixed seeds to ensure reproducibility across all experiments.

\paragraph{Experimental Tasks.}
We conducted three primary experiments to analyze recommendation performance:

\begin{enumerate}
    \item \textbf{Size vs. Runtime Experiment:}
    \begin{itemize}
        \item Fixed edge probability: \(p = 0.1\).
        \item Varied network sizes: \(n \in \{50, 100, 200, 500, 1000\}\).
        \item Measured execution time for:
        \begin{itemize}
            \item \texttt{jaccard\_similarity\_all\_pairs()}: Computing Jaccard for all non-adjacent pairs.
            \item \texttt{recommend()}: Generating recommendations for a single user.
            \item \texttt{recommend\_all()}: Generating recommendations for all users.
        \end{itemize}
        \item Purpose: Analyze computational scalability with network size.
    \end{itemize}
    
    \item \textbf{Quality Experiment:}
    \begin{itemize}
        \item Fixed network: \(n = 500\) nodes, \(p = 0.1\).
        \item Varied test fractions: fraction of edges held out for testing \(\in \{0.1, 0.2, 0.3\}\).
        \item Varied \(k\) values: number of recommendations \(\in \{5, 10, 20\}\).
        \item Evaluated using precision@k, recall@k, and hit rate@k.
        \item Purpose: Measure recommendation accuracy under different evaluation scenarios.
    \end{itemize}
    
    \item \textbf{Noise Experiment:}
    \begin{itemize}
        \item Fixed network: \(n = 500\) nodes, \(p = 0.1\).
        \item Fixed test split: 20\% of edges held out.
        \item Varied noise levels: fraction of training edges perturbed \(\in \{0.0, 0.05, 0.1, 0.15, 0.2, 0.3\}\).
        \item Noise consists of equal parts edge removal and edge addition.
        \item Evaluated with \(k = 10\) recommendations.
        \item Purpose: Study robustness to noisy or incomplete data.
    \end{itemize}
\end{enumerate}

\paragraph{Timing Methodology.}
All timings were measured using Python's \texttt{time.perf\_counter()}, which provides high-resolution wall-clock time suitable for performance analysis.

For each experimental configuration:
\begin{enumerate}
    \item Generate the synthetic social network with appropriate parameters.
    \item For each algorithm/operation:
    \begin{enumerate}
        \item Record start time.
        \item Execute the algorithm.
        \item Record end time.
        \item Compute elapsed time: \texttt{end\_time - start\_time}.
    \end{enumerate}
    \item Store all measurements in CSV files for later analysis.
\end{enumerate}

Each measurement represents a single run on a freshly generated network instance.

\paragraph{Evaluation Methodology.}
For the quality and noise experiments, we use a train/test split approach:

\begin{enumerate}
    \item \textbf{Generate network:} Create synthetic social network with \(n\) users and friendship probability \(p\).
    
    \item \textbf{Split edges:} Randomly select a fraction of edges as test set, remove them from the graph to create training set.
    
    \item \textbf{Apply noise (noise experiment only):} Perturb the training graph by randomly adding and removing edges.
    
    \item \textbf{Train recommender:} Initialize \texttt{FriendRecommender} with the training graph and personality tags.
    
    \item \textbf{Generate recommendations:} For each user, produce top-\(k\) friend recommendations.
    
    \item \textbf{Evaluate against test set:} Compare recommendations to held-out test edges.
    
    \item \textbf{Compute metrics:}
    \begin{itemize}
        \item For each user \(u\), let \(R_u\) be the set of recommended friends and \(T_u\) be the set of actual new friends from the test set.
        \item Precision@k for \(u\): \(|R_u \cap T_u| / k\).
        \item Recall@k for \(u\): \(|R_u \cap T_u| / |T_u|\).
        \item Hit for \(u\): 1 if \(|R_u \cap T_u| \geq 1\), else 0.
        \item Aggregate by averaging over all users.
    \end{itemize}
\end{enumerate}

This methodology simulates real-world deployment where recommendations are made based on current network state and evaluated against future friendship formation.

\paragraph{Reproducibility.}
All experimental code is located in \texttt{experiments/recommender/}:

\begin{itemize}
    \item \texttt{run\_experiments.py}: Main script containing:
    \begin{itemize}
        \item \texttt{run\_recommender\_size\_experiment()}
        \item \texttt{run\_recommender\_quality\_experiment()}
        \item \texttt{run\_recommender\_noise\_experiment()}
    \end{itemize}
    
    \item Results are saved to \texttt{experiments/recommender/results/}:
    \begin{itemize}
        \item \texttt{size\_results.csv}: Runtime scaling data.
        \item \texttt{quality\_results.csv}: Precision, recall, hit rate for different test fractions and \(k\) values.
        \item \texttt{noise\_results.csv}: Metrics under varying noise levels.
    \end{itemize}
\end{itemize}

The experiments can be reproduced by running:
\begin{verbatim}
python experiments/recommender/run_experiments.py
\end{verbatim}

\paragraph{Hardware and Software Environment.}
All experiments were conducted in a consistent environment:

\begin{itemize}
    \item \textbf{Hardware:} Standard computing environment (specific specs may vary).
    \item \textbf{Operating System:} Windows.
    \item \textbf{Python Version:} Python 3.x.
    \item \textbf{Libraries:} Standard library only for core algorithms; \texttt{matplotlib} for visualization (not used in algorithm execution).
\end{itemize}

\paragraph{Parameter Choices.}
Our experimental parameters were chosen to balance:

\begin{itemize}
    \item \textbf{Network sizes:} \(n \leq 1000\) for reasonable computation time while demonstrating scalability trends.
    \item \textbf{Edge probability:} \(p = 0.1\) creates sparse graphs typical of real social networks (average degree \(\sim 10\%\) of network size).
    \item \textbf{Number of tags:} 1--3 tags per user from a pool of 7 provides realistic diversity without overwhelming similarity.
    \item \textbf{Test fractions:} 10--30\% mimics realistic train/test splits in link prediction literature.
    \item \textbf{Noise levels:} 0--30\% covers mild to severe data quality issues.
    \item \textbf{Recommendation size:} \(k \in \{5, 10, 20\}\) represents practical recommendation list sizes.
\end{itemize}

\paragraph{Expected Theoretical Behavior.}
Based on our complexity analysis:

\begin{itemize}
    \item \textbf{Runtime scaling:}
    \begin{itemize}
        \item All-pairs Jaccard: \(O(n^2 \bar{d})\) - quadratic growth expected.
        \item Single user recommendation: \(O(n \bar{d})\) - linear in \(n\) for fixed \(p\).
        \item All users: \(O(n^2 \bar{d})\) - quadratic growth expected.
    \end{itemize}
    
    \item \textbf{Quality metrics:}
    \begin{itemize}
        \item Precision decreases as \(k\) increases (more recommendations means lower precision).
        \item Recall increases as \(k\) increases (more opportunities to hit test edges).
        \item Hit rate increases with \(k\) (easier to get at least one correct).
        \item Larger test fractions provide more test edges, potentially increasing recall.
    \end{itemize}
    
    \item \textbf{Noise impact:}
    \begin{itemize}
        \item Edge removal degrades graph structure, reducing common neighbor signals.
        \item Edge addition introduces spurious similarities.
        \item Moderate noise should have limited impact due to tag similarity providing complementary signal.
        \item Severe noise (\(> 20\%\)) should noticeably degrade performance.
    \end{itemize}
\end{itemize}

The experimental results section will compare these predictions with empirical observations.

\paragraph{Plotting and Visualization.}
All visualizations are generated using \texttt{matplotlib}. Plotting scripts are located in \texttt{plots/recommender/} and generate figures from CSV result files, ensuring that visualization tools do not influence algorithmic measurements.
