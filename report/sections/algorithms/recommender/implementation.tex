\subsubsection{Implementation Details}

In this project, all recommendation algorithms were implemented in Python using object-oriented design principles and efficient data structures for scalability.

\paragraph{Data Structures.}
The recommender system operates on two primary data structures:

\begin{itemize}
    \item \textbf{Graph representation:} Dictionary-based adjacency list mapping user IDs to lists of friend IDs:
    \[
    \texttt{graph} : V \to \text{list of neighbors}.
    \]
    
    \item \textbf{Tag representation:} Dictionary mapping user IDs to lists of personality tags:
    \[
    \texttt{tags} : V \to \text{list of tag strings}.
    \]
\end{itemize}

For efficiency, the \texttt{FriendRecommender} class precomputes:
\begin{itemize}
    \item \textbf{Neighbor sets:} Convert adjacency lists to Python \texttt{set} objects for \(O(1)\) average-case membership testing and efficient set operations.
    \item \textbf{Tag inverted index (optional):} For the inverted index optimization, we build:
    \[
    \texttt{tag\_index} : \text{tag} \to \text{list of users with that tag}.
    \]
    This allows fast lookup of users sharing a specific tag.
\end{itemize}

\paragraph{Implementation Choices.}
Several design decisions balance performance, maintainability, and flexibility:

\begin{itemize}
    \item \textbf{Set operations for Jaccard:}
    Python's built-in \texttt{set} type provides optimized intersection (\texttt{\&}) and union (\texttt{|}) operations using hash tables.
    For sets of size \(d_u\) and \(d_v\), these operations run in \(O(d_u + d_v)\) expected time, which is optimal for the Jaccard computation.
    
    \item \textbf{Object-oriented design:}
    The \texttt{FriendRecommender} class encapsulates:
    \begin{itemize}
        \item State: graph, tags, weights, precomputed neighbor sets.
        \item Methods: \texttt{recommend()}, \texttt{recommend\_all()}, \texttt{tag\_similarity()}, etc.
    \end{itemize}
    This design allows easy instantiation with different configurations and promotes code reuse.
    
    \item \textbf{Configurable weights:}
    The recommendation score weights (\(w_1, w_2, w_3\)) are passed as a dictionary at initialization, allowing experiments with different weight configurations without modifying code.
    Default values: \texttt{\{"jaccard": 0.4, "tags": 0.3, "adamic\_adar": 0.3\}}.
    
    \item \textbf{Adamic-Adar normalization:}
    Since the raw Adamic-Adar index is unbounded, we normalize using:
    \[
    S_{\text{AA}} = \frac{AA}{1 + AA},
    \]
    which maps \([0, \infty) \to [0, 1)\) and ensures all components have comparable ranges.
    
    \item \textbf{Three recommendation strategies:}
    We implement three methods for generating recommendations:
    \begin{enumerate}
        \item \texttt{recommend\_all()} - Naive \(O(n^2)\) approach for small graphs.
        \item \texttt{recommend\_all\_inverted\_index()} - Optimized using friend-of-friend and tag index lookups.
        \item \texttt{recommend\_all\_sparse\_matrix()} - Matrix multiplication approach for larger graphs.
    \end{enumerate}
    This allows empirical comparison of optimization strategies.
    
    \item \textbf{Excluding existing friends:}
    The \texttt{exclude\_friends} parameter (default \texttt{True}) ensures recommendations don't include users who are already friends.
    This is the typical use case but can be disabled for testing.
\end{itemize}

\paragraph{Code Organization.}
The recommendation algorithms are organized in \texttt{algorithms/recommender/}:

\begin{itemize}
    \item \texttt{jaccard.py}:
    \begin{itemize}
        \item \texttt{jaccard\_similarity()} - Compute Jaccard for a single pair.
        \item \texttt{jaccard\_similarity\_all\_pairs()} - Compute for all non-adjacent pairs.
        \item \texttt{common\_neighbors()}, \texttt{adamic\_adar\_index()}, etc. - Additional link prediction metrics.
    \end{itemize}
    
    \item \texttt{friend\_recommender.py}:
    \begin{itemize}
        \item \texttt{FriendRecommender} class - Main recommendation engine.
        \item \texttt{recommend\_friends()} - Convenience function for single-user recommendations.
        \item \texttt{evaluate\_recommendations()} - Train/test evaluation harness.
    \end{itemize}
\end{itemize}

Each function includes comprehensive docstrings with:
\begin{itemize}
    \item Parameter and return type specifications.
    \item Time and space complexity analysis.
    \item Usage examples.
    \item Mathematical definitions where applicable.
\end{itemize}

\paragraph{Evaluation Framework.}
To assess recommendation quality, we implement a train/test split framework:

\begin{enumerate}
    \item \textbf{Edge splitting:}
    Use \texttt{split\_edges\_for\_testing()} from \texttt{graph/noise.py} to randomly remove a fraction of edges (e.g., 20\%) for testing.
    The remaining edges form the training graph.
    
    \item \textbf{Recommendation generation:}
    Train the recommender on the training graph (with test edges removed) and generate top-\(k\) recommendations for each user.
    
    \item \textbf{Metric computation:}
    Compare recommendations against held-out test edges to compute precision, recall, and hit rate.
    
    \item \textbf{Aggregation:}
    Average metrics across all users to obtain overall performance measures.
\end{enumerate}

This evaluation paradigm mimics real-world scenarios where we predict future friendships based on current network structure.

\paragraph{Noise Injection.}
To study robustness, we use \texttt{apply\_noise()} to perturb the training graph:

\begin{itemize}
    \item \textbf{Edge removal:} Randomly delete a fraction of edges to simulate incomplete data.
    \item \textbf{Edge addition:} Randomly add non-existent edges to simulate false connections or spam accounts.
\end{itemize}

Noise level \(\epsilon\) specifies the total fraction of edges to perturb, split equally between additions and removals.
For example, \(\epsilon = 0.1\) means 5\% edges removed and 5\% edges added.

\paragraph{Optimization Benchmarking.}
In the demonstration script (\texttt{friend\_recommender.py}), we include a benchmark comparing the three recommendation strategies:

\begin{itemize}
    \item Generate a graph with 500 nodes and random tags.
    \item Run all three methods and measure wall-clock time.
    \item Verify that all methods produce identical results (up to tie-breaking).
    \item Report speedup factors relative to the naive baseline.
\end{itemize}

On sparse graphs (\(p \approx 0.05\)), the inverted index and sparse matrix approaches typically achieve 5--10Ã— speedups compared to the naive approach.

\paragraph{Testing and Validation.}
All implementations were validated through:

\begin{itemize}
    \item \textbf{Unit tests:} Small hand-crafted examples with known correct recommendations.
    \item \textbf{Symmetry checks:} Verify \(J(u, v) = J(v, u)\) and \(S_{\text{tag}}(u, v) = S_{\text{tag}}(v, u)\).
    \item \textbf{Boundary tests:} Check behavior when users have no friends or no tags.
    \item \textbf{Consistency checks:} Compare results of different optimization strategies to ensure they match.
    \item \textbf{Integration tests:} Run full recommendation pipeline on synthetic networks and verify reasonable precision/recall values.
\end{itemize}

\paragraph{Extensibility.}
The modular design facilitates easy extension:

\begin{itemize}
    \item \textbf{New similarity metrics:} Add methods like \texttt{geographic\_similarity()} and include in the weighted sum.
    \item \textbf{Machine learning integration:} Use computed features as input to a learned ranking model.
    \item \textbf{Dynamic updates:} Incrementally update neighbor sets and inverted index when new edges are added.
    \item \textbf{Personalized weights:} Learn per-user weights based on their interaction history.
\end{itemize}

This flexibility makes the system suitable for research experimentation and real-world deployment.
